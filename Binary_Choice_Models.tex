%% LyX 2.4.2.1 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{beamer}
\usepackage{mathpazo}
\renewcommand{\familydefault}{\rmdefault}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage[active]{srcltx}
\usepackage{amsthm}
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}
\theoremstyle{definition}
\newtheorem*{example*}{\protect\examplename}
\theoremstyle{definition}
\newtheorem*{defn*}{\protect\definitionname}
\theoremstyle{plain}
\newtheorem*{thm*}{\protect\theoremname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }\usepackage[english]{babel}
\usepackage{babel}

%\usetheme{Boadilla}
\usetheme{Madrid}
% \usecolortheme{orchid}
\usecolortheme{spruce}
% \usecolortheme{beaver}

\setbeamercovered{transparent}

\usepackage{colortbl}

\usefonttheme[onlymath]{serif}
%%%%%%%%%%%%%%%%%%%%%%%%

% For tables
\usepackage{multirow}
\usepackage{array}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{float}
\usepackage{booktabs}


% For figures
\usepackage{caption}
\usepackage{subcaption}

\makeatother

\usepackage{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title[BCM]{Binary Choice Models}
\author[]{Zhentao Shi}
\date[]{}
\makebeamertitle
\begin{frame}{Binary Choice Models}
    \begin{itemize}
        \item Classification
        \item Discrete random variables
    \end{itemize}
\end{frame}
\begin{frame}{Examples}
    \[y_{i} = 1 \, \text{or}\,0\]
    \begin{itemize}
        \item College entrance
        \item Marriage
        \item Job hunting
    \end{itemize}
\end{frame}
\begin{frame}{Linear Regression}
    \[y_i = x_i^{'} \beta + \varepsilon_i\]
    \[E\left[y_{i} = 1 \mid X_{i}\right] = X_{i}^{'} \beta\]
    \[\Rightarrow \Pr\left(\varepsilon _{i} = 1 - X_{i}^{'} \beta \mid X_i \right) = \Pr \left( y _{i} = 1 \mid x_i \right) = X_i^{'} \beta\]
    \[E\left(y_i = 0 \mid X_i \right) = 1 - X_i^{'} \beta\]
    \[\Rightarrow \Pr \left(\varepsilon _i = 0 - X_i^{'} \beta \mid X_{i} \right) = \Pr \left( y_{i} = 0 \right) = 1 - X_{i} \beta\]
    \[\operatorname{Var} \left( \varepsilon_{i} \mid X_{i} \right)= X_{i} ^{'} \beta \left( 1 - X_i^{'} \beta \right)\]    
    varies with $X_i$, heteroskedastic
    
\end{frame}
\begin{frame}{Predicted Range}
    $E\left(y_{i} = 1 \mid X_{i} \right) = X_{i}^{'} \beta$ can go beyond $\left[0,1\right]$
    \begin{itemize}
        \item $X_{i}^{'} \beta$ is a "single index"
    \end{itemize}
    to ensure natural probability,
    \[E\left( y _{i} = 1 \mid X_{i} \right) = G\left(X_i^{'} \beta\right)\]
    for some $G(\cdot ) : \mathbb{R} \to \left( 0, 1\right]$
    \begin{itemize}
        \item popular choice
        \begin{itemize}
            \item $G \sim \text{normal cdf}$
            \item $G\left(x\right) = \frac{1}{1 + \exp\left(-x\right)}$logistic cdf
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{Facts about Logistic CDF}
    \[\Lambda = \Lambda \left(x\right) = \frac{1}{1 + \exp \left(-x\right)}\]
    \[- \Lambda ^{2} = -2 \Lambda -\Lambda \left(1 - \Lambda \right)\]
    \[\frac{\mathrm{d} \Lambda }{\mathrm{d} X} = \Lambda \left(1-\Lambda \right)\]
    
\end{frame}
\begin{frame}{Latent aliling Model}
    \[y^{*} = X_{i}^{'} \beta + \varepsilon _{i}\]
    \[y = \mathbb{I}\left\{ y_{i} ^{*} \ge 0 \right\}\]
    Assume $\varepsilon _{i} \mid X_i \sim \text{logistic distribution}$
    \begin{align*}
        \Pr\left(y_i = 1 \mid X_i \right) &= \Pr \left(X_i^{'} \beta + \varepsilon _{i} \ge 0 \mid X_i\right)\\
        & = \Pr \left( - \varepsilon _{i} \le X_{i}^{'} \beta \mid X_{i}\right)\\
        & = \Lambda \left(X_i^{'} \beta\right)
    \end{align*}
\end{frame}
\begin{frame}{Estimating}
    \begin{align*}
        L(\beta) &= \prod_{y_i = 1 } \Lambda \left( X_i ^{'} \beta \right)\cdot \prod_{y_i = 0} \left(1 - \Lambda \left(X_{i}^{'} \beta \right)\right)\\
        &= \prod_{i =1}^{N}\left\{ \Lambda \left(X_i^{'} \beta \right) \right\} ^{y_i} \left\{ 1- \Lambda \left( X_{i}^{'} \beta \right) \right\} ^{1-y_{i}}
    \end{align*}
    \[\log L\left( \beta \right) = \sum_{i =1}^{N} \left\{ y_i \log \left( \Lambda \left( X_{i}^{'}\beta \right) \right) + \left( 1 - y_{n} \right)  \log \left( 1 - \Lambda \left( X_i \beta \right) \right)\right\}\]
    
    
\end{frame}

\begin{frame}{Marginal Effects}
    \begin{align*}
        S_N(\beta) &= \sum_{i = 1}^{N}\left\{ \frac{y_i}{\Lambda_i}\cdot \Lambda_{i}\left( 1 - \Lambda _i \right) X_{i} - \frac{\left( 1- y_{i} \right)}{1 - \Lambda_{i}} \cdot \Lambda_{i} \left( 1-\Lambda_{i} \right) X_{i}\right\}\\
        &= \sum_{i=1}^{N}\left\{ y_{i} \left( 1- \Lambda_{i} \right) - \left( 1-y_{i} \right) \Lambda_{i}\right\} X_{i}\\
        &= \sum_{i =1}^{N}\left( y_{i} - \Lambda _{i} \right)X_{i} 
    \end{align*}
    \[\frac{\partial L \left( \beta \right)}{\partial \beta \partial \beta ^{'}} = - \sum_{i=1}^{N} \Lambda_{i}\left( 1 - \Lambda_{i} \right) X_{i} Y_{i}\]
    is negative definite
    \begin{itemize}
        \item globally concave
        \item unique maximizer
    \end{itemize}
\end{frame}
\begin{frame}{Goodness of Fit (for genetic classification)}
    MCFadden $R^{2} = 1- \log L_{1} / \log L_{0}$
    \begin{itemize}
        \item $\log L_{1}$: maximum of likelihood
        \item $\log L_{0}$: the null model (no $X$, intercept only)
            \[\log L_{0} = N_1 \log \hat{p_{1}} + \left( N-N_{1} \right)\log \left( 1- \hat{p_1} \right) \]
            where $\hat{p_{1}} = N_{1} / N$
        \item $\log L_{0} < \log L_{1} < 0 \Rightarrow \frac{\log L_{1}}{\log L_{0}} \in \left[0,1  \right]$                
    \end{itemize}
\end{frame}
\begin{frame}{Prediction}
    \[\hat{y_{i}} = 1 \,\text{if}\, \Pr\left( y_{i} \mid X_{i} \right) \ge 0.5\]
    \begin{table}
        \centering
        \begin{tabular}{cc|ccc}
            & & $\hat{y_{i}}$ & & \\
            & & $0$ & $1$ & Total \\
            \hline
            $y_{i}$ & $0$ & $n_{00}$ & $n_{01}$ & $N_{0}$ \\
            & $1$ & $n_{10}$ & $n_{11}$ & $N_{1}$ \\
            &&&& $N$ 
        \end{tabular}
    \end{table}
\end{frame}



\end{document}