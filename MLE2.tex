%% LyX 2.4.2.1 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{beamer}
\usepackage{mathpazo}
\renewcommand{\familydefault}{\rmdefault}
\usepackage[T1]{fontenc}
% \usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage[active]{srcltx}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}
\theoremstyle{definition}
\newtheorem*{example*}{\protect\examplename}
\theoremstyle{definition}
\newtheorem*{defn*}{\protect\definitionname}
\theoremstyle{plain}
\newtheorem*{thm*}{\protect\theoremname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }\usepackage[english]{babel}
\usepackage{babel}

%\usetheme{Boadilla}
\usetheme{Madrid}
\setbeamertemplate{navigation symbols}{}
% \usecolortheme{orchid}
\usecolortheme{spruce}
% \usecolortheme{beaver}

\setbeamercovered{transparent}

\usepackage{colortbl}

\usefonttheme[onlymath]{serif}
%%%%%%%%%%%%%%%%%%%%%%%%

% For tables
\usepackage{multirow}
\usepackage{array}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{float}
\usepackage{booktabs}


% For figures
\usepackage{caption}
\usepackage{subcaption}

\makeatother

\usepackage{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\theoremname}{Theorem}

\begin{document}
\title[MLE]{Maximum Likelihood}
\author[]{Zhentao Shi}
\date[]{The Chinese University of Hong Kong}
\makebeamertitle






\begin{frame}{Likelihood}


\begin{itemize}

    \item The most likely outcome
    \item Distributional assumption 
\end{itemize}
\begin{center}
    \includegraphics[width=0.45\textwidth]{fig/L(theta).png}
\end{center}
\end{frame}



\begin{frame}{Model Specification}
\begin{itemize}
\item Nature: Data $z$ is drawn from a parameter model $f$
\item Human: specify a family of models $g\left(z;\theta\right)$ and a parameter space $\Theta$, which span a \textbf{model space} $G\left(\Theta\right)=\left\{ g\left(z;\theta\right):\theta\in\Theta\right\} $.
\end{itemize}
\begin{center}
    \includegraphics[width=0.8\textwidth]{fig/misspecified.png}
\end{center}
\end{frame}




\begin{frame}{Model and Specification}


\textbf{Parametric model}. The distribution of the data $\mathbf{Y}=\left(Y_{1},...,Y_{N}\right)$
is known up to a finite dimensional parameter.


\bigskip


\begin{itemize}
\item  \textbf{Semiparametric model}: If we know $Y\sim i.i.d.\left(\mu,\sigma^{2}\right)$,
we can estimate $\mu,\sigma^{2}$ by method of moments.
\item \textbf{Parametric model}: If we assume $Y\sim N\left(\mu,\sigma^{2}\right)$,
the model has only two parameters $\mu$ and $\sigma^{2}$. 
\end{itemize}

\end{frame}

\begin{frame}{Likelihood Function}
\begin{itemize}
\item For simplicity, let $\mathbf{Y}=(Y_{1},\ldots,Y_{N})$ be i.i.d.  
\item The \textbf{likelihood} of the sample under a hypothesized value of
$\theta\in\Theta$ is
\[
L\left(\theta;\mathbf{Y}\right)=f\left(\mathbf{Y};\theta\right)=\prod_{i=1}^{N}f\left(Y_{i};\theta \right)
\]

\item Two perspectives:
\begin{itemize}
    \item (Probabilist) $f(\mathbf{Y};\theta)$ is a function of $\mathbf{Y}$ given the parameter $\theta$
    \item (Statistician) $L(\theta;\mathbf{Y})$ is a function of $\theta$ given the data $\mathbf{Y}$


\end{itemize}

\end{itemize}
\end{frame}



\section{Correct Specification}
\frame{\sectionpage}




\begin{frame}{Log-likelihood}
\begin{itemize}
\item log-likelihood
\[
\ell_{N}\left(\theta\right)
= \log L\left(\theta;\mathbf{Y}\right)
= \sum_{i=1}^{N}\log f\left(Y_{i};\theta\right)
\]
is easier to compute.

\item $\log(\cdot)$ is a monotonically increasing function

\item The MLE estimator
\[
\hat{\theta}=\arg\max_{\theta\in\varTheta}\ell_{N}\left(\theta\right)
\]
\end{itemize}
\end{frame}
%
\begin{frame}{Why Maximization: Deep Justification}
\begin{thm*}
If the model is correctly specified, then $\theta_{0}$ is the maximizer.
\end{thm*}

\begin{itemize}
    \item Kullback-Leibler information criterion (KLIC):
\[
KLIC\left(f,g\right)=\int f\left(z\right)\log\frac{f\left(z\right)}{g\left(z\right)}dz
\]

    \item $KLIC \geq 0$ because 
\begin{align*}
& E\left[\log f\left(Y;\theta_{0}\right)\right]-E\left[\log f\left(Y;\theta\right)\right] \\ 
= & E\left[\log\left(f\left(Y;\theta_{0}\right)/f\left(Y;\theta\right)\right)\right]\\
= & -E\left[\log\left(f\left(Y;\theta\right)/f\left(Y;\theta_{0}\right)\right)\right]\\
\geq & -\log E\left[f\left(Y;\theta\right)/f\left(Y;\theta_{0}\right)\right]=0
\end{align*}
by the Jensen's inequality.

\end{itemize}

\end{frame}
\begin{frame}{KLIC}
    \begin{center}
        \includegraphics[width=\textwidth]{fig/KLIC.png}
    \end{center}
\end{frame}


\begin{frame}{Score and Hessian}

\begin{itemize}
\item Score $s_{N}(\theta)=\sum_{i=1}^{N}\frac{\partial}{\partial\theta}\log f(Y_{i};\theta)$ is a function of $\theta$

\item Efficient score $s_{i0}=\frac{\partial}{\partial\theta}\log f\left(Y_{i};\theta_{0}\right)$ is evaluated at the true value $\theta_0$

 

\end{itemize}

\begin{thm*}
If the model is correctly specified, the support of $Y$ does not
depend on $\theta$, and $\theta_{0}$ is in the interior of $\Theta$,
then $E\left[s_{i0}\right]=0$.
\end{thm*}


MLE is equivalent to looking for roots of $s_N(\theta) = 0$.
 

\bigskip

\begin{itemize}

\item Hessian: $H_{N}(\theta)=-\sum_{i=1}^{N}\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f(Y_{i};\theta)$

\item Expected Hessian:
$
H_{0}=-E\left[\frac{\partial^{2}}{\partial\theta\partial\theta'}\log f\left(Y;\theta_{0}\right)\right]
$
\end{itemize}
\end{frame}



\begin{frame}{Score and Hessian: Illustration}
    \begin{center}
        \includegraphics[width=\textwidth]{fig/KLIC.png}
    \end{center}
\end{frame}




\begin{frame}{Information Equality}
\begin{itemize}
\item \textbf{Fisher Information Matrix}: $I_{0}=E[s_{i0}s_{i0}']$ 
\end{itemize}
%
\begin{thm*}
If the model is correctly specified, the support of $Y$ does not
depend on $\theta$, and $\theta_{0}$ is in the interior of $\Theta$,
then $$I_{0}=H_{0}.$$
\end{thm*}
\begin{itemize}

\item Information equality fails when the model is misspecified
\end{itemize}
\end{frame}


\begin{frame}{Cram\'{e}r-Rao Lower Bound}
\begin{thm*}
Suppose the model is correctly specified, the support of $Y$ does
not depend on $\theta$, and $\theta_{0}$ is in the interior of $\Theta$.
If $\widetilde{\theta}$ is unbiased estimator, then $$var(\widetilde{\theta})\geq\left(NI_{0}\right)^{-1}.$$
\end{thm*}

\bigskip

\begin{itemize}
\item More general than ``BLUE''
\item A lower bound for variance of unbiased estimator
\item When reached, an estimator is called \textbf{Cram\'{e}r-Rao efficient}.
\end{itemize}
\end{frame}


\begin{frame}{Normal Regression}

\begin{itemize}
    \item The normal regression models is 
$$Y_{i}=X_{i}'\beta+\varepsilon_{i}$$
     \item Under the assumption $\varepsilon_{i} \mid X_{i}\sim N \left(0,\gamma \right),$ the conditional distribution is 
     $$Y_{i} \mid X_{i} \sim N \left( X_{i}'\beta,\gamma\right).$$

    \item Parameter $\theta = (\beta, \gamma)$
    \item The joint likelihood $$f(Y_i , X_i) = f(Y_i | X_i) f(X_i),$$ where the specification of $f(X_i)$ is irrelevant to $\theta$.
\end{itemize}
\end{frame}


\begin{frame}{Asymptotic Normality}
\begin{itemize}

\item Under regularity conditions, $\hat{\theta}\stackrel{p}{\to}\theta_{0}$, and
\[
\sqrt{N}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,H_{0}^{-1}I_{0}H_{0}^{-1}\right)
\]
\item When the information equality holds, we have 
\[
\sqrt{N}\left(\hat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,I_{0}^{-1}\right),
\]
or equivalently
\[
\hat{\theta}-\theta_{0} \stackrel{a}{\sim}N\left(0,\frac{I_{0}^{-1}}{N}\right),
\]
\item The variance $(N I_0)^{-1}$ is efficient!

\end{itemize}


\end{frame}




\section{Mispecification}
\frame{\sectionpage}



\begin{frame}[plain]{KLIC for Misspecified Models}

\begin{center}
    \includegraphics[width=0.7\textwidth]{fig/misspecified.png}
\end{center}
\begin{itemize}
\item If $f\notin G\left(\Theta\right)$, the model is  misspecified. 
\begin{align*}
KLIC\left(f,g\left(z;\theta\right)\right) & =\int f\left(z\right)\log f\left(z\right)dz-\int f\left(z\right)\log g\left(z;\theta\right)dz\\
 & =E\left[\log f\left(z\right)\right]-E\left[\log g\left(z;\theta\right)\right] > 0
\end{align*}

\end{itemize}
\end{frame}




\begin{frame}{Misspecified Model}
\begin{itemize}

\item Misspecified: $\min_{\theta\in\Theta}KLIC\left(f,g\left(z;\theta\right)\right)>0$
\item MLE is still meaningful

\item Pseudo-true parameter:
\[
\theta^{*}=\arg\max_{\theta\in\Theta} E [\ell\left(\theta\right)]
\]
the minimizer of $KLIC\left(f,g\left(z;\theta\right)\right)$ in the
parameter space $\Theta$
\item Under standard assumption, the MLE estimator $\widehat{\theta}\stackrel{p}{\to}\theta^{*}$ and 
\[
\sqrt{N}\left(\hat{\theta}-\theta^{*}\right)\stackrel{d}{\to}N\left(0,H_{*}^{-1}I_{*}H_{*}^{-1}\right)
\]
%
\end{itemize}
\end{frame}


\begin{frame}{Summary}

\begin{itemize}
    \item Parametric models
    \item Specification of distribution family
    \item MLE
    \item Score, Hessian, information matrix
    \item Misspecification
\end{itemize}
    
\end{frame}



% \begin{frame}{My Related Research}
%     \begin{itemize}
%         \item Ming Li, Zhentao Shi, and Yapeng Zheng (2024) ``Estimation and Inference in Dyadic Network Formation Models with Nontransferable Utilities'', \emph{working paper}. \url{https://arxiv.org/abs/2410.23852}
%         \item Jinyuan Chang, Zhentao Shi and Jia Zhang (2023): ``Culling the Herd of Moments with Penalized Empirical Likelihood,'' \emph{Journal of Business \& Economic Statistics}, 41(3), 791-805. \url{https://doi.org/10.1080/07350015.2022.2071903}
%         \item Zhentao Shi (2016): ``Econometric Estimation with High-Dimensional Moment Equalities,'' \emph{Journal of Econometrics}, 195, 104-119. \url{https://doi.org/10.1016/j.jeconom.2016.07.004}
%     \end{itemize}
    
 

\end{document}


% to continue with limited dependent variables